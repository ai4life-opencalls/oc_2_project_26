{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2bd973a",
   "metadata": {},
   "source": [
    "This notebook aims to test different registration methods using the Ground Truth points that were manually detected on EM and LM images.\n",
    "\n",
    "#### Loading the Ground truth files in XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "70880430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import bigfish.stack as stack\n",
    "import bigfish.plot as plot\n",
    "from utils import xml_to_dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cd189a",
   "metadata": {},
   "source": [
    "Load the locations for fiducial particles in EM and LM. The locations were manually detected using ICY software and were saved in XML format. These point locations are considered to be the ground truth. For further processing we need to load the XML format and convert it into Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514f3a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_xml = 'Ground_truth_fiducials_EM.xml'\n",
    "target_xml = 'Ground_truth_fiducials_EM_only_clusters.xml'\n",
    "target_df = xml_to_dataframe(target_xml)\n",
    "print(target_df)\n",
    "\n",
    "#source_xml = 'Ground_truth_fiducials_LM.xml'\n",
    "source_xml = 'Ground_truth_fiducials_LM_only_clusters.xml'\n",
    "source_df_small = xml_to_dataframe(source_xml)\n",
    "print(source_df_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9364d2b",
   "metadata": {},
   "source": [
    "EM and LM images have most probably different sizes. LM image is usually much smaller. The points that were detected in LM image coordinate system needs to be rescaled to EM image coordinate system, otherwise it might be very hard with the registration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fdee3f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_the_scale(EM_shape, LM_shape):\n",
    "    scale_x = EM_shape[0]/LM_shape[0]\n",
    "    scale_y = EM_shape[1]/LM_shape[1]\n",
    "    return scale_x, scale_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52a1d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the EM and LM images\n",
    "\n",
    "input_path = \"E:/DATA/AI4Life_Pr26/20240805_Trial_data_fiducial_particles/240723_JB294_CLEM-AI4life_sample1/pos1\"\n",
    "\n",
    "EM_image_path = os.path.join(input_path, \"240726_JB295_HEK293_CLEM_LAMP1-488_Particles-555_grid4_pos1_bin4_EM.tif\")\n",
    "LM_image_path  = os.path.join(input_path, \"240726_JB295_HEK293_CLEM_LAMP1-488_Particles-555_grid4_pos1_LM.tif\")\n",
    "\n",
    "EMimage = stack.read_image(EM_image_path)\n",
    "LMimage_small = stack.read_image(LM_image_path)\n",
    "\n",
    "\n",
    "# Find what is the scaling rate between the 2 images\n",
    "scale_x, scale_y = find_the_scale(EMimage.shape, LMimage_small.shape)\n",
    "\n",
    "print(\"Scale x: \",scale_x)\n",
    "print(\"Scale y: \",scale_y)\n",
    "\n",
    "\n",
    "# Resize the LM point positions\n",
    "source_df = xml_to_dataframe(source_xml)\n",
    "source_df['pos_x'] = source_df_small['pos_x']*scale_x\n",
    "source_df['pos_y'] = source_df_small['pos_y']*scale_y\n",
    "\n",
    "\n",
    "# Resize the LM image to fit the position of the resized points\n",
    "LMimage = stack.resize_image(LMimage_small, EMimage.shape, method='bilinear')\n",
    "\n",
    "#scale_x = 24.873456790123456\n",
    "#scale_y = 22.966165413533833"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58466bb",
   "metadata": {},
   "source": [
    "Converting the Panda dataframe into numpy array (2D points (X,Y) and 3D points (X,Y,Z)). The 3D points are needed if we plan to convert them to point clouds later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3aec0e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting dataframe into numpy array of coordinate pairs (X,Y) \n",
    "target = target_df[['pos_x', 'pos_y']].to_numpy()\n",
    "source = source_df[['pos_x', 'pos_y']].to_numpy()\n",
    "source_small = source_df_small[['pos_x', 'pos_y']].to_numpy()\n",
    "\n",
    "# Adding the Z-dimension to the 2D points\n",
    "target3D = np.hstack((target, np.zeros((len(target), 1))))\n",
    "source3D = np.hstack((source, np.zeros((len(source), 1))))\n",
    "source3D_small = np.hstack((source_small, np.zeros((len(source_small), 1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67631d3",
   "metadata": {},
   "source": [
    "Plotting the point coordinates on top of the image to double check the correct placement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5555cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot.plot_detection(LMimage_small[:,:,1], source_small, contrast=True)\n",
    "plot.plot_detection(LMimage[:,:,1], source, shape=\"circle\", radius = 3*scale_y, color = \"red\", linewidth = 1, fill=False, contrast=True) \n",
    "plot.plot_detection(EMimage, target, radius = 3*scale_y, contrast=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c325a4",
   "metadata": {},
   "source": [
    "Converting points in numpy array (X,Y,Z) into points clouds and saving them as .ply files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83def0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "# Convert NumPy array to Open3D PointCloud\n",
    "target_pcd = o3d.geometry.PointCloud()\n",
    "target_pcd.points = o3d.utility.Vector3dVector(target3D)\n",
    "source_pcd = o3d.geometry.PointCloud()\n",
    "source_pcd.points = o3d.utility.Vector3dVector(source3D)\n",
    "\n",
    "# Visualize the point cloud\n",
    "#o3d.visualization.draw_geometries([target_pcd])\n",
    "#o3d.visualization.draw_geometries([source_pcd])\n",
    "\n",
    "# Assuming 'point_cloud' is your Open3D PointCloud object\n",
    "o3d.io.write_point_cloud(\"target.ply\", target_pcd)\n",
    "o3d.io.write_point_cloud(\"source.ply\", source_pcd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813826ef",
   "metadata": {},
   "source": [
    "### Testing different registration methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e44b71b",
   "metadata": {},
   "source": [
    "Loading source and target point clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9d5655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "target_pcd = o3d.io.read_point_cloud('target.ply')\n",
    "source_pcd = o3d.io.read_point_cloud('source.ply')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac8db11",
   "metadata": {},
   "source": [
    "Registration method: Coherent point drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66f93107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from probreg import cpd\n",
    "\n",
    "# Perform CPD registration\n",
    "tf_param, _, _ = cpd.registration_cpd(source_pcd, target_pcd)\n",
    "\n",
    "# Apply the transformation to the source point cloud\n",
    "transformed_source_cpd = tf_param.transform(source_pcd.points)\n",
    "\n",
    "# Visualize the results\n",
    "source_pcd.paint_uniform_color([1, 0, 0])  # Red for source\n",
    "target_pcd.paint_uniform_color([0, 1, 0])  # Green for target\n",
    "transformed_source_cpd_pcd = o3d.geometry.PointCloud()\n",
    "transformed_source_cpd_pcd.points = o3d.utility.Vector3dVector(transformed_source_cpd)\n",
    "transformed_source_cpd_pcd.paint_uniform_color([0, 0, 1])  # Blue for result\n",
    "\n",
    "o3d.visualization.draw_geometries([source_pcd, target_pcd, transformed_source_cpd_pcd])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc15cfb3",
   "metadata": {},
   "source": [
    "Testing different registration methods provided by Probreg package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "efd965ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "def visualize_result_nparray(source, target, result, title):\n",
    "    source_cloud = o3d.geometry.PointCloud()\n",
    "    source_cloud.points = o3d.utility.Vector3dVector(source)\n",
    "    source_cloud.paint_uniform_color([1, 0, 0])  # Red\n",
    "\n",
    "    target_cloud = o3d.geometry.PointCloud()\n",
    "    target_cloud.points = o3d.utility.Vector3dVector(target)\n",
    "    target_cloud.paint_uniform_color([0, 1, 0])  # Green\n",
    "\n",
    "    result_cloud = o3d.geometry.PointCloud()\n",
    "    result_cloud.points = o3d.utility.Vector3dVector(result)\n",
    "    result_cloud.paint_uniform_color([0, 0, 1])  # Blue\n",
    "\n",
    "    o3d.visualization.draw_geometries([source_cloud, target_cloud, result_cloud], window_name=title)\n",
    "\n",
    "def visualize_result_pcd(source, target, result, title):\n",
    "    source.paint_uniform_color([1, 0, 0])  # Red\n",
    "    target.paint_uniform_color([0, 1, 0])  # Green\n",
    "    result.paint_uniform_color([0, 0, 1])  # Blue\n",
    "    o3d.visualization.draw_geometries([source, target, result], window_name=title)\n",
    "\n",
    "# Print transformations\n",
    "def print_transformations(transformation_paramters, title):\n",
    "    print(title)\n",
    "    print(transformation_paramters.rot)  # Rotation matrix\n",
    "    print(transformation_paramters.t)    # Translation vector\n",
    "    print(transformation_paramters.scale)  # Scale factor\n",
    "\n",
    "    # Evaluate alignment using chamfer distance\n",
    "def chamfer_distance(A, B, title):\n",
    "    distances = np.min(np.sum((A[:, np.newaxis, :] - B[np.newaxis, :, :]) ** 2, axis=2), axis=1)\n",
    "    print(f\"Chamfer distance = {np.mean(distances)} ({title}) \")\n",
    "\n",
    "    return np.mean(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d967e09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from probreg import cpd, gmmtree, filterreg, bcpd, l2dist_regs\n",
    "\n",
    "# Load or create your point cloud data\n",
    "source = o3d.io.read_point_cloud(\"source.ply\")\n",
    "target = o3d.io.read_point_cloud(\"target.ply\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "source_points = np.asarray(source.points)\n",
    "target_points = np.asarray(target.points)\n",
    "\n",
    "# 1. Coherent Point Drift (CPD)\n",
    "# tf_param_cpd, _, _ = cpd.registration_cpd(source_points, target_points)\n",
    "# result_cpd = tf_param_cpd.transform(source_points)\n",
    "\n",
    "# 1. Coherent Point Drift (CPD)\n",
    "## Rigid CPD - WORKS\n",
    "tf_param_rigid, result_rigid2, result_rigid3 = cpd.registration_cpd(source_points, target_points, tf_type_name='rigid', maxiter=500, tol=1e-5)\n",
    "result_rigid = tf_param_rigid.transform(source_points)\n",
    "\n",
    "'''\n",
    "## Affine CPD - Error - LinAlgError: Singular matrix\n",
    "#tf_param_affine, _, _ = cpd.registration_cpd(source_points, target_points, tf_type_name='affine')\n",
    "#result_affine = tf_param_affine.transform(source_points)\n",
    "#print(\"Affine CPD Transformation:\")\n",
    "#print(tf_param_affine.b)  # Affine matrix\n",
    "#print(tf_param_affine.t)  # Translation vector\n",
    "\n",
    "acpd = cpd.AffineCPD(source, use_cuda=False)\n",
    "tf_param_affine, result_nonrigid2, _ = acpd.registration(target)\n",
    "result_nonrigid = tf_param_affine.transform(source_points)\n",
    "print(\"Affine CPD Transformation:\")\n",
    "print(tf_param_affine.b)  # Affine matrix\n",
    "print(tf_param_affine.t)  # Translation vector\n",
    "\n",
    "## Non-rigid CPD - WORKS, but NO TRANSFORMATION\n",
    "tf_param_nonrigid, _, _ = cpd.registration_cpd(source_points, target_points, tf_type_name='nonrigid', maxiter=200, tol=1e-5)\n",
    "result_nonrigid = tf_param_nonrigid.transform(source_points)\n",
    "print(\"Non-rigid CPD Transformation:\")\n",
    "print(tf_param_nonrigid.g)  # Displacement field\n",
    "print(tf_param_nonrigid.w)  # Weight matrix\n",
    "'''\n",
    "\n",
    "# 2. FilterReg - WORKS\n",
    "tf_param_filterreg, _, _ = filterreg.registration_filterreg(source_points, target_points)\n",
    "result_filterreg = tf_param_filterreg.transform(source_points)\n",
    "#print('RESULTS:', result_filterreg)\n",
    "#print('RESULTS2:',result_filterreg2)\n",
    "\n",
    "'''\n",
    "# 3. GMMReg (Gaussian Mixture Model Registration) Error - TypeError: cannot unpack non-iterable RigidTransformation object\n",
    "#tf_param_gmmreg, _ = l2dist_regs.registration_gmmreg(source_points, target_points)\n",
    "#result_gmmreg = tf_param_gmmreg.transform(source_points)\n",
    "\n",
    "# 4. SVR (Support Vector Registration) Error - InvalidParameterError: The 'gamma' parameter of OneClassSVM must be a str among {'scale', 'auto'} or a float in the range [0.0, inf). Got np.float64(inf) instead.\n",
    "#tf_param_svr, _ = l2dist_regs.registration_svr(source_points, target_points)\n",
    "#result_svr = tf_param_svr.transform(source_points)\n",
    "\n",
    "# 5. GMMTree (Hierarchical Stochastic Model) - WORKS, but NO TRANSFORMATION\n",
    "tf_param_gmmtree, _ = gmmtree.registration_gmmtree(source_points, target_points)\n",
    "result_gmmtree = tf_param_gmmtree.transform(source_points)\n",
    "\n",
    "# 6. Bayesian Coherent Point Drift (BCPD) (experimental) - Error: cannot unpack non-iterable CombinedTransformation object\n",
    "#tf_param_bcpd, _ = bcpd.registration_bcpd(source_points, target_points)\n",
    "#result_bcpd = tf_param_bcpd.transform(source_points)\n",
    "'''\n",
    "\n",
    "# Visualize results for each method\n",
    "#visualize_result(source_points, target_points, result_cpd, \"CPD Registration\")\n",
    "visualize_result_nparray(source_points, target_points, result_rigid, \"Rigid CPD\")\n",
    "#visualize_result(source_points, target_points, result_affine, \"Affine CPD\")\n",
    "#visualize_result(source_points, target_points, result_nonrigid, \"Non-rigid CPD\")\n",
    "#visualize_result(source_points, target_points, result_gmmreg, \"GMMReg\")\n",
    "#visualize_result(source_points, target_points, result_svr, \"SVR\")\n",
    "#visualize_result(source_points, target_points, result_gmmtree, \"GMMTree\")\n",
    "visualize_result_nparray(source_points, target_points, result_filterreg, \"FilterReg\")\n",
    "#visualize_result(source_points, target_points, result_bcpd, \"BCPD\")\n",
    "\n",
    "# Print transformations\n",
    "print_transformations(tf_param_rigid, \"Rigid CPD Transformation:\")\n",
    "print_transformations(tf_param_filterreg, \"FilterReg Transformation:\")\n",
    "#print_transformations(tf_param_gmmreg, \"GMMReg Transformation:\")\n",
    "#print_transformations(tf_param_svr, \"\"SVR Transformation:\"\")\n",
    "#print_transformations(tf_param_gmmtree, \"GMMTree Transformation:\")\n",
    "#print_transformations(result_bcpd, \"BCPD Transformation:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed5d552",
   "metadata": {},
   "source": [
    "Open3D library that provides ICP (Iterative Closest Point) registration algorithms - point-to-point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948f6967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "# Load or create your point cloud data and initial alignment\n",
    "source = o3d.io.read_point_cloud(\"source.ply\")\n",
    "target = o3d.io.read_point_cloud(\"target.ply\")\n",
    "threshold = 10000\n",
    "trans_init = np.identity(4)\n",
    "\n",
    "# Point-to-point ICP\n",
    "print(\"Applying point-to-point ICP\")\n",
    "reg_p2p = o3d.pipelines.registration.registration_icp( source, target, threshold, trans_init, o3d.pipelines.registration.TransformationEstimationPointToPoint())\n",
    "\n",
    "print(reg_p2p)\n",
    "print(\"Transformation is: \\n\", reg_p2p.transformation)\n",
    "\n",
    "result_p2p = copy.deepcopy(source)\n",
    "result_p2p.transform(reg_p2p.transformation)\n",
    "\n",
    "# Visualize the result\n",
    "visualize_result_pcd(source, target, result_p2p , \"point-to-point ICP\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5511200",
   "metadata": {},
   "outputs": [],
   "source": [
    "chamfer_distance(target_points, source_points, \"No transform\")\n",
    "chamfer_distance(target_points, result_rigid,\"Rigid CPD Transformation\")\n",
    "chamfer_distance(target_points, result_filterreg, \"FilterReg Transformation\")\n",
    "chamfer_distance(target_points, np.asarray(result_p2p.points), \"point-to-point ICP\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI4Life_OC2_2024_26",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
